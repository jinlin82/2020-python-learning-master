---
title: "Pandas 用法"
author: "Jin"
date: "2020年6月"
institute: 中南财经政法大学统计与数学学院
csl: ./style/chinese-gb7714-2015-numeric.csl
css: ./style/markdown.css
bibliography: [./Bibfile.bib]
eqnPrefixTemplate: ($$i$$)
link-citations: true
linkReferences: true
chapters: true
tableEqns: false
autoEqnLabels: false
classoption: "aspectratio=1610"
---

```{r setup, echo=F, purl=F}
knitr::opts_knit$set(root.dir = getwd())
knitr::opts_chunk$set(echo = TRUE, results = 'hide')
knitr::opts_chunk$set(warning = FALSE, message=FALSE)
knitr::opts_chunk$set(fig.align="center"
                      ## ,out.width="0.9\\textwidth" # latex
                      ,out.width="80%" # for both latex and html
                      ,fig.width=5, fig.height=3
                      )
```

```{r prepare, echo=F, purl=F}
rm(list=ls())
options(digits=4)
options(scipen=100)
graphics.off()
Sys.setlocale("LC_ALL", "Chinese")
library(reticulate)
```




# 简介

### 基本情况

1.  原作者: Wes McKinney
2.  初始版本: 2008
3.  稳定版本: 0.19.1 / November 3, 2016;
4.  网址: <http://pandas.pydata.org>

### 什么是Pandas?

1.  pandas是一个为Python编程编写的软件库数据处理和分析语言。
2.  特别是，它提供了用于操纵数值表和时间序列的数据结构和操作。
3.  它的目标是成为在Python中进行实际的、真实的数据分析的基本高层构建块。
4.  此外，它还有一个更广泛的目标：成为任何语言中可用的最强大和最灵活的开源数据分析/操作工具。

### 库功能

1.  pandas的两个主要数据结构Series（一维）和DataFrame（二维），可处理金融、统计、社会科学和许多工程领域中的典型用例。
2.  2.对于R用户，DataFrame提供R的data.frame提供的所有内容以及其他更多内容。
3.  pandas运行快。 许多低级算法位已在Python代码中进行了广泛的调整。 但是，与其他方法一样，泛化通常会牺牲性能。
4.  pandas是statsmodels的依赖项，使其成为Python统计计算生态系统的重要组成部分。
5.  pandas已被广泛用于金融应用的生产中。

### pandas擅长之处

1.  易于处理浮点数据和非浮点数据中的缺失数据（表示为NaN）
2.  大小可变性：可以从DataFrame和更高维度的对象中插入和删除列
3.  自动和显式的数据对齐：可以将对象显式地对齐到一组标签，或者用户可以简单地忽略标签，让Series，DataFrame等在计算中自动对齐数据
4.  强大、灵活的分组依据功能，可对数据集执行“拆分-应用-合并”操作，用于聚合和转换数据
5.  使之更容易地将Python和NumPy数据结构中不规则、不同索引的数据转换为DataFrame对象

### pandas擅长之处

1.  [@6]直观地合并和连接数据集
2.  基于智能标签的切片、花式索引和大型数据集的子集划分
3.  数据集的灵活整形和旋转
4.  轴的分层标记（每个刻度可能有多个标签）
5.  强大的IO工具，用于从平面文件（CSV和分隔符）、Excel文件、数据库加载数据，以及从超高速HDF5格式保存/加载数据
6.  时间序列特定功能：日期范围生成和频率转换，移动窗口统计，移动窗口线性回归，日期转移和滞后等

### pandas由以下元素组成

1.  一组带标签的数组数据结构，其主要是Series和DataFrame
2.  索引对象，支持简单轴索引和多级/层次轴索引
3.  集成的分组引擎，用于聚合和转换数据集
4.  日期范围生成（Date~range~）和自定义日期偏移，可实现自定义频率
5.  输入/输出工具：从平面文件（CSV、delimited、excel2003）加载表格数据，并保存和加载
6.  快速高效的PyTables / HDF5格式的pandas对象
7.  内存效率高的标准数据结构的“稀疏”版本，用于存储大部分缺失或基本不变的数据（某些固定值）
8.  移动窗口统计（滚动平均值、滚动标准差等）
   

# 输入/输出数据

### CSV

1.  写入csv文件: `df.to_csv('foo.csv')`
2.  从csv文件中读取: `pd.read_csv('foo.csv')`

### Excel

1.  写入excel文件:
    `df.to_excel('foo.xlsx', sheet_name='Sheet1')`
2.  从excel文件中读取:
    `pd.read_excel('foo.xlsx', 'Sheet1', index_col=None, na_values=['NA'])`

### 对象创建

1.  通过传递值列表创建序列，让pandas创建一个默认的整数索引：

``` {.python}
import pandas as pd
import numpy as np
pd.Series([1, 2, 5, np.nan, 8])
```

1.  通过传递带有日期时间索引和标记列的numpy数组创建Dataframe：

``` {.python}
dates = pd.date_range('20130101', periods=6)
df = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list('ABCD'))
```

1.  通过传递可以转换为类似序列的对象的dict来创建DataFrame：

``` {.python}
df2 = pd.DataFrame({ 'A':1.,
                     'B':pd.Timestamp('20130102'),
                     'C' : pd.Series(1,index=list(range(4)),dtype='float32'),
                     'D' : np.array([3]*4,dtype='int32'),
                     'E' : pd.Categorical(["test","train","test","train"]),
                     'F' : 'foo' })

df2.dtypes

```

### 查看数据

1.  查看数据框的顶行和底行
    1.  df.head()
    2.  df.tail(3)
2.  显示索引、列和底层numpy数据
    1.  df.index
    2.  df.columns
    3.  df.values
3.  描述显示数据的快速统计摘要
    1.  df.describe()
4.  转换数据
    1.  df.T

### 排序

1.  按轴排列 `df.sort_index(axis=1, ascending=False)`
2.  按值排列 `df.sort_values(by='B')`

# 选取

### 使用中括号[ ]选取

1.  选择单个列，这会产生一个序列，相当于df.A

`df['A']`

1.  选择连续行

`df[2:4]`

### 使用标签（label）选取： `.loc[ ]`

1.  必须都是使用标签，不是数字
2.  标签可以使用冒号
3.  DataFrame 使用两个下标，中间用逗号分开，全选的要使用冒号
4.  可以用于非连续行列选取

``` {.python}
df2 = pd.DataFrame(np.random.randn(5,4), columns=list('ABCD'),
                   index=pd.date_range('20130101',periods=5))

df2.loc[2:3] ### 错误
df2.loc['20130102':'20130104']
df2.loc[:,['A','C']]
```

### 使用位置选取： `.iloc[ ]`

 `.iloc` 属性是主要的访问方法，以下是有效的输入：

1.  一个整数：5
2.  整数的列表或数组：[4, 3, 0]
3.  一个由整数组成的切片对象：1:7
4.  一个布尔值数组
5.  当切片时，起始边界被包含，而上界被排除

``` {.python}
df2 = pd.DataFrame(np.random.randn(5,4), columns=list('ABCD'),
                   index=pd.date_range('20130101',periods=5))

df2.iloc[2]
df2.iloc[2,3]
df2.iloc[[0,2,4]]
df2.iloc[[0,2,4], :3]

```

### `DataFrame.at` 和 `DataFrame.iat`

1.  `DataFrame.at` :访问行/列标签对的单个值
2.  `DataFrame.iat` :按整型位置访问行/列对的单个值

### 逻辑值下标

1.  运算符为: | 表示或， & 表示且， ~ 表示否
2.  必须使用括号对它们进行分组。
3.  使用布尔向量索引序列的工作原理与numpy ndarray完全相同

``` {.python}
###Using a single column’s values to select data.
df[df.A > 0]
df[df > 0]

df2.loc[df2['A'] > 0, 'A':'C']

df2.iloc[list(df2['A'] > 0), 0:3]

```

### 用`isin`索引

1.  考虑一下序列的
    isin方法，它返回一个布尔向量，该向量在传递的列表中存在序列元素的任何地方都为
    真。

``` {.python}
s = pd.Series(np.arange(5), index=np.arange(5)[::-1], dtype='int64')
s[s.isin([2,4,6])]
```

1.  DataFrame也有一个“isin”方法。当调用isin时，以数组
    或dict的形式传递一组值。
2.  如果值是一个数组，isin返回一个由布尔数组构成的DataFrame，其形状与原始的DataFrame相同。

``` {.python}
df = pd.DataFrame({'vals': [1, 2, 3, 4], 'ids': ['a', 'b', 'f', 'n'],
                    'ids2': ['a', 'n', 'c', 'n']})
values = ['a', 'b', 1, 3]
df.isin(values)
```

### 用`isin`索引

1.  通常，希望将某些值与某些列匹配。只需将值设置为字典，其中键是列，而值是要检查的项的列表。

``` {.python}
values = {'ids': ['a', 'b'], 'vals': [1, 3]}
df.isin(values)
```

1.  将DataFrame的isin与' any() '和' all() '方法结合起来，快速选择满足给定条件的数据子集。选择一个每列都满足自己条件的行:

``` {.python}
values = {'ids': ['a', 'b'], 'ids2': ['a', 'c'], 'vals': [1, 3]}
row_mask = df.isin(values).all(1)
df[row_mask]
```

### `where()`方法

1.  从具有布尔向量的序列中选择值通常返回数据的一个子集。
2.  为了保证选择输出与原始数据具有相同的形状，可以使用Series和DataFrame中的where方法。
3.  此外，where接受一个可选的其他参数，用于替换返回副本中条件为假的值。
4.  注意：DataFrame.where()方法的签名与numpy.where()不同。大概地，df1.where(m,
    df2)和np.where(m, df1, df2)相同。

``` {.python}
s[s > 0]
s.where[s>0]
df.where(df < 0, -df)
```

# 重复数据和缺失值

### 重复数据

1.  如果你想要识别和删除一个DataFrame中的重复行，有两个方法可以帮助:`duplicated` 和
    `drop_duplicates`.
2.  每个都将用于标识重复行的列作为参数。
3.  duplicated返回一个布尔向量，其长度为行数，并显示一行是否是重复的。
4.  drop_duplicates 删除重复的行。
5.  默认情况下，重复集的第一个观察行被认为是唯一的，但是每个方法都有一个keep参数来指定要保留的目标。
    -   keep='first' (default): 标记/删除重复，除了第一次出现的。
    -   keep='last': 标记/删除重复，除了最后一次出现的。
    -   keep=False: 标记/删除所有重复。

### Duplicate Data

``` {.python}
df2 = pd.DataFrame({'a': ['one', 'one', 'two', 'two', 'two', 'three', 'four'],
                    'b': ['x', 'y', 'x', 'y', 'x', 'x', 'x'],
                    'c': np.random.randn(7)})

df2.duplicated('a')
df2.duplicated('a', keep='last')
df2.duplicated('a', keep=False)
df2.drop_duplicates('a')
df2.drop_duplicates('a', keep='last')
df2.drop_duplicates('a', keep=False)
### pass a list of columns to identify duplications.
df2.duplicated(['a', 'b'])
df2.drop_duplicates(['a', 'b'])
```

### 缺失值

1.  pandas主要使用np.nan表示丢失的数据。默认情况下，它不包括在计算中。
2.  删除任何缺少数据的行。`df1.dropna(how='any')`
3.  填充缺失的数据：`df1.fillna(value=5)`
4.  获取值为nan的布尔值： `pd.isna(df1)`

# Operations

### 统计

1.  Performing a descriptive statistic:
    -   `df.mean()`
    -   `df.mean(1)`
    -   `np.mean(np.array(df))`

### apply 函数

1.  Applying functions to the data:
    -   `df.apply(np.cumsum)`
2.  `df.apply(lambda x: x.max() - x.min())`

### 字符处理

1.  Series is equipped with a set of string processing methods in the
    str attribute that make it easy to operate on each element of the
    array.

``` {.python}
s = pd.Series(['A', 'B', 'C', 'Aaba', 'Baca', np.nan, 'CABA', 'dog', 'cat'])
s.str.lower()
```

# Merge

### `concat`

1.  Concatenate pandas objects along a particular axis with optional set
    logic along the other axes.
2.  The concat() function (in the main pandas namespace) does all of the
    heavy lifting of performing concatenation operations along an axis
    while performing optional set logic (union or intersection) of the
    indexes (if any) on the other axes.
3.  Like its sibling function on ndarrays, numpy.concatenate,
    pandas.concat takes a list or dict of homogeneously-typed objects
    and concatenates them with some configurable handling of "what to do
    with the other axes"

``` {.python}
df = pd.DataFrame(np.random.randn(10, 4))
pieces = [df[:3], df[3:7], df[7:]]
pd.concat(pieces)
```

### `appdend`

1.  A useful shortcut to concat() are the append() instance methods on
    Series and DataFrame.

```{=html}
<!-- -->
```
1.  These methods actually predated concat. They concatenate along
    axis=0, namely the index.

``` {.python}
df = pd.DataFrame(np.random.randn(8, 4), columns=['A','B','C','D'])
s = df.iloc[3]
df.append(s)
df.append(s, ignore_index=True)

```

### `join`

1.  pandas has full-featured, high performance in-memory join operations
    idiomatically very similar to relational databases like SQL.
2.  These methods perform significantly better (in some cases well over
    an order of magnitude better) than other open source implementations
    (like `base::merge.data.frame` in R).
3.  The reason for this is careful algorithmic design and the internal
    layout of the data in DataFrame.
4.  pandas provides a single function, `merge()`, as the entry point for
    all standard database join operations between DataFrame or named
    Series objects
5.  the related `join()` method, uses merge internally for the
    index-on-index (by default) and column(s)-on-index join.

# Grouping

### Group By: split-apply-combine

-   By "group by" we are referring to a process involving one or more of
    the following steps:
    1.  Splitting the data into groups based on some criteria.
    2.  Applying a function to each group independently.
    3.  Combining the results into a data structure.
-   Out of these, the split step is the most straightforward. In fact,
    in many situations we may wish to split the data set into groups and
    do something with those groups.

### apply step

-   Aggregation: compute a summary statistic (or statistics) for each
    group. Some examples:
    1.  Compute group sums or means.
    2.  Compute group sizes / counts.
-   Transformation: perform some group-specific computations and return
    a like-indexed object. Some examples:
    1.  Standardize data (zscore) within a group.
    2.  Filling NAs within groups with a value derived from each group.
-   Filtration: discard some groups, according to a group-wise
    computation that evaluates True or False. Some examples:
    1.  Discard data that belongs to groups with only a few members.
    2.  Filter out data based on the group sum or mean.
-   Some combination of the above: GroupBy will examine the results of
    the apply step and try to return a sensibly combined result if it
    doesn't fit into either of the above two categories.

### 例子

``` {.python}
df = pd.DataFrame({'A': ['foo', 'bar', 'foo', 'bar',
                           'foo', 'bar', 'foo', 'foo'],
                     'B': ['one', 'one', 'two', 'three',
                           'two', 'two', 'one', 'three'],
                     'C': np.random.randn(8),
                     'D': np.random.randn(8)})

df.groupby('A').sum()
df.groupby(['A','B']).sum()


```

## Aggregation

### Applying multiple functions at once

1.  With grouped Series you can also pass a list or dict of functions to
    do aggregation with, outputting a DataFrame.
2.  On a grouped DataFrame, you can pass a list of functions to apply to
    each column, which produces an aggregated result with a hierarchical
    index.

``` {.python}
df.groupby(['A','B']).agg([np.sum, np.mean, np.std])

```

### Applying different functions to DataFrame columns

1.  By passing a dict to aggregate you can apply a different aggregation
    to the columns of a DataFrame:

``` {.python}
grouped.agg({'C' : np.sum,
               'D' : lambda x: np.std(x, ddof=1)})

```

### Group Plotting

1.  Groupby also works with some plotting methods.
2.  For example, suppose we suspect that some features in a DataFrame
    may differ by group, in this case, the values in column 1 where the
    group is "B" are 3 higher on average.

``` {.python}
import matplotlib.pyplot as plt
np.random.seed(1234)
df = pd.DataFrame(np.random.randn(50, 2))
df['g'] = np.random.choice(['A', 'B'], size=50)
df.loc[df['g'] == 'B', 1] += 3
df.groupby('g').boxplot()
```

# Reshaping

### 使用 `pivot()` 方法

``` {.python}
import pandas.util.testing as tm; tm.N = 3
def unpivot(frame):
   N, K = frame.shape
   data = {'value' : frame.values.ravel('F'),
           'variable' : np.asarray(frame.columns).repeat(N),
           'date' : np.tile(np.asarray(frame.index), K)}
   return pd.DataFrame(data, columns=['date', 'variable', 'value'])

df = unpivot(tm.makeTimeDataFrame())

df[df['variable'] == 'A']
df.pivot(index='date', columns='variable', values='value')
df['value2'] = df['value']*2
df.pivot('date', 'variable')

pivoted = df.pivot('date', 'variable')
pivoted['value2']

```

### Pivot tables

1.  While pivot provides general purpose pivoting of DataFrames with
    various data types (strings, numerics, etc.),
2.  the pivot~table~ function for pivoting with aggregation of numeric
    data.
3.  The function pandas.pivot~table~ can be used to create
    spreadsheet-style pivot tables.

### Pivot tables

1.  pandas.pivot~table~ takes a number of arguments
    -   data: A DataFrame object
    -   values: a column or a list of columns to aggregate
    -   index: a column, Grouper, array which has the same length as
        data, or list of them. Keys to group by on the pivot table
        index. If an array is passed, it is being used as the same
        manner as column values.
    -   columns: a column, Grouper, array which has the same length as
        data, or list of them. Keys to group by on the pivot table
        column. If an array is passed, it is being used as the same
        manner as column values.
    -   aggfunc: function to use for aggregation, defaulting to
        numpy.mean
2.  pass margins=True to pivot~table~, special All columns and rows will
    be added with partial group aggregates across the categories on the
    rows and columns

### Pivot tables

``` {.python}
df = pd.DataFrame({"A": ["foo", "foo", "foo", "foo", "foo",
                         "bar", "bar", "bar", "bar"],
                   "B": ["one", "one", "one", "two", "two",
                         "one", "one", "two", "two"],
                   "C": ["small", "large", "large", "small",
                         "small", "large", "small", "small",
                         "large"],
                   "D": [1, 2, 2, 3, 3, 4, 5, 6, 7],
                   "E": [2, 4, 5, 5, 6, 6, 8, 9, 9]})

table = pd.pivot_table(df, values='D', index=['A', 'B'],
                    columns=['C'], aggfunc=np.sum)

table = pd.pivot_table(df, values=['D', 'E'], index=['A', 'C'],
                    aggfunc={'D': np.mean,
                             'E': np.mean})

table = pd.pivot_table(df, values=['D', 'E'], index=['A', 'C'],
                    aggfunc={'D': np.mean,
                             'E': [min, max, np.mean]})
```

### Cross tabulations

1.  Use the crosstab function to compute a cross-tabulation of two (or
    more) factors.
2.  By default crosstab computes a frequency table of the factors unless
    an array of values and an aggregation function are passed.
3.  arguments:
    -   index: array-like, values to group by in the rows
    -   columns: array-like, values to group by in the columns
    -   values: array-like, optional, array of values to aggregate
        according to the factors
    -   aggfunc: function, optional, If no values array is passed,
        computes a frequency table
    -   rownames: sequence, default None, must match number of row
        arrays passed
    -   colnames: sequence, default None, if passed, must match number
        of column arrays passed
    -   margins: boolean, default False, Add row/column margins
        (subtotals)
    -   normalize: boolean, {'all', 'index', 'columns'}, or {0,1},
        default False. Normalize by dividing all values by the sum of
        values.

### Cross tabulations

``` {.python}
foo, bar, dull, shiny, one, two = 'foo', 'bar', 'dull', 'shiny', 'one', 'two'
a = np.array([foo, foo, bar, bar, foo, foo], dtype=object)
b = np.array([one, one, two, one, two, one], dtype=object)
c = np.array([dull, dull, shiny, dull, dull, shiny], dtype=object)
pd.crosstab(a, [b, c], rownames=['a'], colnames=['b', 'c'])

df = pd.DataFrame({'A': [1, 2, 2, 2, 2], 'B': [3, 3, 4, 4, 4],
                   'C': [1, 1, np.nan, 1, 1]})
pd.crosstab(df.A, df.B)
pd.crosstab(df['A'], df['B'], normalize=True)
pd.crosstab(df['A'], df['B'], normalize='columns')
pd.crosstab(df.A, df.B, values=df.C, aggfunc=np.sum, normalize=True,
margins=True)

```

### `cut` function

1.  The cut function computes groupings for the values of the input
    array and is often used to transform continuous variables to
    discrete or categorical variables.
2.  If the bins keyword is an integer, then equal-width bins are formed.
    Alternatively we can specify custom bin-edges

### `cut` function

``` {.python}
ages = np.array([10, 15, 13, 12, 23, 25, 28, 59, 60])
pd.cut(ages, bins=3)
pd.cut(ages, bins=[0, 18, 35, 70])
```

### dummy variables: get~dummies~()

1.  get~dummies~() converts a categorical variable into a "dummy" or
    "indicator" DataFrame,
2.  for example a column in a DataFrame (a Series) which has k distinct
    values, can derive a DataFrame containing k columns of 1s and 0s
3.  get~dummies~() also accepts a DataFrame. By default all categorical
    variables (categorical in the statistical sense, those with object
    or categorical dtype) are encoded as dummy variables.
4.  control the columns that are encoded with the columns keyword.
5.  drop~first~ keyword only keep k-1 levels of a categorical variable
    to avoid collinearity.

### dummy variables: get~dummies~()

``` {.python}
df = pd.DataFrame({'key': list('bbacab'), 'data1': range(6)})
pd.get_dummies(df['key'])
dummies = pd.get_dummies(df['key'], prefix='key')

df = pd.DataFrame({'A': ['a', 'b', 'a'], 'B': ['c', 'c', 'b'],
                   'C': [1, 2, 3]})

pd.get_dummies(df['key'])
pd.get_dummies(df, columns=['A'])
```

### 其他函数

1.  stack unstack
2.  melt
3.  wide~tolong~

# Categoricals

### 创建分类

1.  By specifying dtype=\"category\" when constructing a Series
2.  pandas can include categorical data in a DataFrame by convert
3.  Rename the categories to more meaningful names (assigning to
    Series.cat.categories
4.  Reorder the categories and simultaneously add the missing categories
5.  Sorting is per order in the categories, not lexical order.
6.  Grouping by a categorical column shows also empty categories.

### 例子

``` {.python}
s = pd.Series(["a","b","c","a"], dtype="category")
df = pd.DataFrame({"id":[1,2,3,4,5,6], "raw_grade":['a', 'b', 'b', 'a', 'a', 'e']})
df["grade"] = df["raw_grade"].astype("category")
df["grade"].cat.categories = ["very good", "good", "very bad"]
df["grade"] = df["grade"].cat.set_categories(["very bad", "bad", "medium", "good", "very good"])

df.sort_values(by="grade")
df.groupby("grade").size()

```

# Plotting

### `plot()`

1.  The plot method on Series and DataFrame is just a simple wrapper
    around plt.plot()

```{=html}
<!-- -->
```
1.  On DataFrame, plot() is a convenience to plot all of the columns
    with labels
2.  plot one column versus another using the x and y keywords in plot()

``` {.python}
ts = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2000',periods=1000))
ts = ts.cumsum()
ts.plot()

df3 = pd.DataFrame(np.random.randn(1000, 2), columns=['B', 'C']).cumsum()
df3['A'] = pd.Series(list(range(len(df))))

df3.plot(x='A', y='B')
```

### Other Plots

1.  Plotting methods allow for a handful of plot styles other than the
    default Line plot. These methods can be provided as the kind keyword
    argument to plot(). These include:
    -   'bar' or 'barh' for bar plots
    -   'hist' for histogram
    -   'box' for boxplot
    -   'kde' or \'density\' for density plots
    -   'area' for area plots
    -   'scatter' for scatter plots
    -   'hexbin' for hexagonal bin plots
    -   'pie' for pie plots

### 其他画图函数

1.  These functions can be imported from pandas.plotting and take a
    Series or DataFrame as an argument.
2.  Scatter Matrix Plot： pandas.plotting.scatter~matrix~
3.  create density plots using the Series.plot.kde() and
    DataFrame.plot.kde() methods.
4.  Andrews curves allow one to plot multivariate data as a large number
    of curves that are created using the attributes of samples as
    coefficients for Fourier series: andrews~curves~

### 其他画图函数

1.  [@5]Parallel coordinates is a plotting technique for plotting
    multivariate data. It allows one to see clusters in data and to
    estimate other statistics visually: parallel~coordinates~
2.  Lag plots are used to check if a data set or time series is random:
    lag~plot~
3.  Autocorrelation Plot: autocorrelation~plot~
4.  Bootstrap plots are used to visually assess the uncertainty of a
    statistic, such as mean, median, midrange, etc: bootstrap~plot~
5.  RadViz is a way of visualizing multi-variate data: radviz

<!-- # 参考文献 -->
[//]: # (\bibliography{Bibfile})
