---
title: "Pandas 用法"
author: "Jin"
date: "2020年6月"
institute: 中南财经政法大学统计与数学学院
csl: ./style/chinese-gb7714-2015-numeric.csl
css: ./style/markdown.css
bibliography: [./Bibfile.bib]
eqnPrefixTemplate: ($$i$$)
link-citations: true
linkReferences: true
chapters: true
tableEqns: false
autoEqnLabels: false
classoption: "aspectratio=1610"
---

```{r setup, echo=F, purl=F}
knitr::opts_knit$set(root.dir = getwd())
knitr::opts_chunk$set(echo = TRUE, results = 'hide')
knitr::opts_chunk$set(warning = FALSE, message=FALSE)
knitr::opts_chunk$set(fig.align="center"
                      ## ,out.width="0.9\\textwidth" # latex
                      ,out.width="80%" # for both latex and html
                      ,fig.width=5, fig.height=3
                      )
```

```{r prepare, echo=F, purl=F}
rm(list=ls())
options(digits=4)
options(scipen=100)
graphics.off()
Sys.setlocale("LC_ALL", "Chinese")
library(reticulate)
```




# 简介

### 基本情况

1.  原作者: Wes McKinney
2.  初始版本: 2008
3.  稳定版本: 0.19.1 / November 3, 2016;
4.  网址: <http://pandas.pydata.org>

### 什么是Pandas?

1.  pandas是一个为Python编程编写的软件库数据处理和分析语言。
2.  特别是，它提供了用于操纵数值表和时间序列的数据结构和操作。
3.  它的目标是成为在Python中进行实际的、真实的数据分析的基本高层构建块。
4.  此外，它还有一个更广泛的目标：成为任何语言中可用的最强大和最灵活的开源数据分析/操作工具。

### 库功能

1.  pandas的两个主要数据结构Series（一维）和DataFrame（二维），可处理金融、统计、社会科学和许多工程领域中的典型用例。
2.  2.对于R用户，DataFrame提供R的data.frame提供的所有内容以及其他更多内容。
3.  pandas运行快。 许多低级算法位已在Python代码中进行了广泛的调整。 但是，与其他方法一样，泛化通常会牺牲性能。
4.  pandas是statsmodels的依赖项，使其成为Python统计计算生态系统的重要组成部分。
5.  pandas已被广泛用于金融应用的生产中。

### pandas擅长之处

1.  易于处理浮点数据和非浮点数据中的缺失数据（表示为NaN）
2.  大小可变性：可以从DataFrame和更高维度的对象中插入和删除列
3.  自动和显式的数据对齐：可以将对象显式地对齐到一组标签，或者用户可以简单地忽略标签，让Series，DataFrame等在计算中自动对齐数据
4.  强大、灵活的分组依据功能，可对数据集执行“拆分-应用-合并”操作，用于聚合和转换数据
5.  使之更容易地将Python和NumPy数据结构中不规则、不同索引的数据转换为DataFrame对象

### pandas擅长之处

1.  [@6]直观地合并和连接数据集
2.  基于智能标签的切片、花式索引和大型数据集的子集划分
3.  数据集的灵活整形和旋转
4.  轴的分层标记（每个刻度可能有多个标签）
5.  强大的IO工具，用于从平面文件（CSV和分隔符）、Excel文件、数据库加载数据，以及从超高速HDF5格式保存/加载数据
6.  时间序列特定功能：日期范围生成和频率转换，移动窗口统计，移动窗口线性回归，日期转移和滞后等

### pandas由以下元素组成

1.  一组带标签的数组数据结构，其主要是Series和DataFrame
2.  索引对象，支持简单轴索引和多级/层次轴索引
3.  集成的分组引擎，用于聚合和转换数据集
4.  日期范围生成（Date~range~）和自定义日期偏移，可实现自定义频率
5.  输入/输出工具：从平面文件（CSV、delimited、excel2003）加载表格数据，并保存和加载
6.  快速高效的PyTables / HDF5格式的pandas对象
7.  内存效率高的标准数据结构的“稀疏”版本，用于存储大部分缺失或基本不变的数据（某些固定值）
8.  移动窗口统计（滚动平均值、滚动标准差等）
   

# 输入/输出数据

### CSV

1.  写入csv文件: `df.to_csv('foo.csv')`
2.  从csv文件中读取: `pd.read_csv('foo.csv')`

### Excel

1.  写入excel文件:
    `df.to_excel('foo.xlsx', sheet_name='Sheet1')`
2.  从excel文件中读取:
    `pd.read_excel('foo.xlsx', 'Sheet1', index_col=None, na_values=['NA'])`

### 对象创建

1.  通过传递值列表创建序列，让pandas创建一个默认的整数索引：

``` {.python}
import pandas as pd
import numpy as np
pd.Series([1, 2, 5, np.nan, 8])
```

1.  通过传递带有日期时间索引和标记列的numpy数组创建Dataframe：

``` {.python}
dates = pd.date_range('20130101', periods=6)
df = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list('ABCD'))
```

1.  通过传递可以转换为类似序列的对象的dict来创建DataFrame：

``` {.python}
df2 = pd.DataFrame({ 'A':1.,
                     'B':pd.Timestamp('20130102'),
                     'C' : pd.Series(1,index=list(range(4)),dtype='float32'),
                     'D' : np.array([3]*4,dtype='int32'),
                     'E' : pd.Categorical(["test","train","test","train"]),
                     'F' : 'foo' })

df2.dtypes

```

### 查看数据

1.  查看数据框的顶行和底行
    1.  df.head()
    2.  df.tail(3)
2.  显示索引、列和底层numpy数据
    1.  df.index
    2.  df.columns
    3.  df.values
3.  描述显示数据的快速统计摘要
    1.  df.describe()
4.  转换数据
    1.  df.T

### 排序

1.  按轴排列 `df.sort_index(axis=1, ascending=False)`
2.  按值排列 `df.sort_values(by='B')`

# 选取

### 使用中括号[ ]选取

1.  选择单个列，这会产生一个序列，相当于df.A

`df['A']`

1.  选择连续行

`df[2:4]`

### 使用标签（label）选取： `.loc[ ]`

1.  必须都是使用标签，不是数字
2.  标签可以使用冒号
3.  DataFrame 使用两个下标，中间用逗号分开，全选的要使用冒号
4.  可以用于非连续行列选取

``` {.python}
df2 = pd.DataFrame(np.random.randn(5,4), columns=list('ABCD'),
                   index=pd.date_range('20130101',periods=5))

df2.loc[2:3] ### 错误
df2.loc['20130102':'20130104']
df2.loc[:,['A','C']]
```

### 使用位置选取： `.iloc[ ]`

 `.iloc` 属性是主要的访问方法，以下是有效的输入：

1.  一个整数：5
2.  整数的列表或数组：[4, 3, 0]
3.  一个由整数组成的切片对象：1:7
4.  一个布尔值数组
5.  当切片时，起始边界被包含，而上界被排除

``` {.python}
df2 = pd.DataFrame(np.random.randn(5,4), columns=list('ABCD'),
                   index=pd.date_range('20130101',periods=5))

df2.iloc[2]
df2.iloc[2,3]
df2.iloc[[0,2,4]]
df2.iloc[[0,2,4], :3]

```

### `DataFrame.at` 和 `DataFrame.iat`

1.  `DataFrame.at` :访问行/列标签对的单个值
2.  `DataFrame.iat` :按整型位置访问行/列对的单个值

### 逻辑值下标

1.  运算符为: | 表示或， & 表示且， ~ 表示否
2.  必须使用括号对它们进行分组。
3.  使用布尔向量索引序列的工作原理与numpy ndarray完全相同

``` {.python}
###Using a single column’s values to select data.
df[df.A > 0]
df[df > 0]

df2.loc[df2['A'] > 0, 'A':'C']

df2.iloc[list(df2['A'] > 0), 0:3]

```

### 用`isin`索引

1.  考虑一下序列的
    isin方法，它返回一个布尔向量，该向量在传递的列表中存在序列元素的任何地方都为
    真。

``` {.python}
s = pd.Series(np.arange(5), index=np.arange(5)[::-1], dtype='int64')
s[s.isin([2,4,6])]
```

1.  DataFrame也有一个“isin”方法。当调用isin时，以数组
    或dict的形式传递一组值。
2.  如果值是一个数组，isin返回一个由布尔数组构成的DataFrame，其形状与原始的DataFrame相同。

``` {.python}
df = pd.DataFrame({'vals': [1, 2, 3, 4], 'ids': ['a', 'b', 'f', 'n'],
                    'ids2': ['a', 'n', 'c', 'n']})
values = ['a', 'b', 1, 3]
df.isin(values)
```

### 用`isin`索引

1.  通常，希望将某些值与某些列匹配。只需将值设置为字典，其中键是列，而值是要检查的项的列表。

``` {.python}
values = {'ids': ['a', 'b'], 'vals': [1, 3]}
df.isin(values)
```

1.  将DataFrame的isin与' any() '和' all() '方法结合起来，快速选择满足给定条件的数据子集。选择一个每列都满足自己条件的行:

``` {.python}
values = {'ids': ['a', 'b'], 'ids2': ['a', 'c'], 'vals': [1, 3]}
row_mask = df.isin(values).all(1)
df[row_mask]
```

### `where()`方法

1.  从具有布尔向量的序列中选择值通常返回数据的一个子集。
2.  为了保证选择输出与原始数据具有相同的形状，可以使用Series和DataFrame中的where方法。
3.  此外，where接受一个可选的其他参数，用于替换返回副本中条件为假的值。
4.  注意：DataFrame.where()方法的签名与numpy.where()不同。大概地，df1.where(m,
    df2)和np.where(m, df1, df2)相同。

``` {.python}
s[s > 0]
s.where[s>0]
df.where(df < 0, -df)
```

# 重复数据和缺失值

### 重复数据

1.  如果你想要识别和删除一个DataFrame中的重复行，有两个方法可以帮助:`duplicated` 和
    `drop_duplicates`.
2.  每个都将用于标识重复行的列作为参数。
3.  duplicated返回一个布尔向量，其长度为行数，并显示一行是否是重复的。
4.  drop_duplicates 删除重复的行。
5.  默认情况下，重复集的第一个观察行被认为是唯一的，但是每个方法都有一个keep参数来指定要保留的目标。
    -   keep='first' (default): 标记/删除重复，除了第一次出现的。
    -   keep='last': 标记/删除重复，除了最后一次出现的。
    -   keep=False: 标记/删除所有重复。

### Duplicate Data

``` {.python}
df2 = pd.DataFrame({'a': ['one', 'one', 'two', 'two', 'two', 'three', 'four'],
                    'b': ['x', 'y', 'x', 'y', 'x', 'x', 'x'],
                    'c': np.random.randn(7)})

df2.duplicated('a')
df2.duplicated('a', keep='last')
df2.duplicated('a', keep=False)
df2.drop_duplicates('a')
df2.drop_duplicates('a', keep='last')
df2.drop_duplicates('a', keep=False)
### pass a list of columns to identify duplications.
df2.duplicated(['a', 'b'])
df2.drop_duplicates(['a', 'b'])
```

### 缺失值

1.  pandas主要使用np.nan表示丢失的数据。默认情况下，它不包括在计算中。
2.  删除任何缺少数据的行。`df1.dropna(how='any')`
3.  填充缺失的数据：`df1.fillna(value=5)`
4.  获取值为nan的布尔值： `pd.isna(df1)`

# 操作

### 统计

1.  进行描述统计：
    -   `df.mean()`
    -   `df.mean(1)`
    -   `np.mean(np.array(df))`

### apply 函数

1.  对数据应用函数：
    -   `df.apply(np.cumsum)`
2.  `df.apply(lambda x: x.max() - x.min())`

### 字符处理

1.  序列配备了一套str属性中的字符串处理方法，使得对数组中每个元素的操作变得非常容易。

``` {.python}
s = pd.Series(['A', 'B', 'C', 'Aaba', 'Baca', np.nan, 'CABA', 'dog', 'cat'])
s.str.lower()
```

# 合并

### `concat`

1.  将pandas对象沿某一特定轴与其他轴上的可选集合逻辑连接起来。
2.  concat()函数（在pandas主命名空间中）沿一个轴执行连接操作的所有繁重工作，
    同时执行其他轴上索引（如果有）的可选集合逻辑（并集或交集）。
3.  同ndarray上的兄弟函数一样，numpy.concatenate，pandas.concat从同类型对象中
    获取一个列表或字典，并通过“如何处理其他轴”的一些可配置的操作将它们连接起来。

``` {.python}
df = pd.DataFrame(np.random.randn(10, 4))
pieces = [df[:3], df[3:7], df[7:]]
pd.concat(pieces)
```

### `appdend`

1.  concat()的一些实用快捷方法是基于序列和数据框的append()实例方法。

```{=html}
<!-- -->
```
1.  这些方法实际上早于concat方法。它们按行（即按索引）连接。

``` {.python}
df = pd.DataFrame(np.random.randn(8, 4), columns=['A','B','C','D'])
s = df.iloc[3]
df.append(s)
df.append(s, ignore_index=True)

```

### `join`

1.  pandas拥有功能齐全，表现出色的内存整合操作，与SQL等关系数据库非常相似。
2.  这些方法比其他开源工具（例如R软件中的 `base::merge.data.frame` ）明显表现得
    更好（在某些情况下远远要好）。
3.  原因是其精密的算法设计和数据框中数据的内部布局。
4.  pandas提供了一个函数， `merge()` ，作为数据框和命名为序列的对象之间所有标准
    数据库连接操作的入口点。
5.  有关 `join()` 的方法, 在内部对索引（默认情况下）和列使用merge作为索引连接。

# Grouping

### Group By: 分割-应用-整合

-   使用 "group by" 我们指的是涉及以下一个或多个步骤的过程：
    1.  依照某些标准将数据分割成多组。
    2.  对各组独立地应用函数。
    3.  将结果整合成一个数据结构。
-   其中，分割步骤是最直接的。实际上，在很多时候我们也许都希望将数据集分割成组，
    并对这些组采取不同的操作。

### 使用步骤

-   聚合：计算各个组的概括性统计信息。一些例子：
    1.  计算组总和或组均值。
    2.  计算组大小或计数。
-   转换：执行一些组特定的计算，并返回一个类似索引的对象。一些例子：
    1.  组内的标准化数据（z分数）。
    2.  用从各组中获取的数据填补组内的缺失值。
-   过滤：依据结果是真或假的分组计算来丢弃一些组。一些例子：
    1.  丢弃属于只有少数个体的组的数据。
    2.  过滤出基于组总和或组均值的数据。
-   以上的一些组合：GroupBy将检查使用步骤的结果，并且如果它不符合上述两个类别中
    任何一个，就尝试返回一个合理的整合结果。

### 例子

``` {.python}
df = pd.DataFrame({'A': ['foo', 'bar', 'foo', 'bar',
                           'foo', 'bar', 'foo', 'foo'],
                     'B': ['one', 'one', 'two', 'three',
                           'two', 'two', 'one', 'three'],
                     'C': np.random.randn(8),
                     'D': np.random.randn(8)})

df.groupby('A').sum()
df.groupby(['A','B']).sum()


```

## 聚合

### 同时应用多个功能

1.  对于分组的系列，你还可以传递一个列表或一组函数来进行聚合，并输出一个数据框。
2.  在一个分组的数据框中，你可以传递一个要应用于每个列的函数列表，该列表将产生一
    个带有分层索引的聚合结果。

``` {.python}
df.groupby(['A','B']).agg([np.sum, np.mean, np.std])

```

### 对数据框的列应用不同的函数

1.  通过传递一个字典进行聚合，你可以对数据框的列应用不同的聚合方式:

``` {.python}
grouped.agg({'C' : np.sum,
               'D' : lambda x: np.std(x, ddof=1)})

```

### 组织策划

1.  Groupby还可以使用一些绘图方法。
2.  例如，假设我们怀疑数据框中的某些特性可能因组而异，在本例中，第1列中组为“B”的
    值要比总体平均值高3。

``` {.python}
import matplotlib.pyplot as plt
np.random.seed(1234)
df = pd.DataFrame(np.random.randn(50, 2))
df['g'] = np.random.choice(['A', 'B'], size=50)
df.loc[df['g'] == 'B', 1] += 3
df.groupby('g').boxplot()
```

# 重塑

### 使用 `pivot()` 方法

``` {.python}
import pandas.util.testing as tm; tm.N = 3
def unpivot(frame):
   N, K = frame.shape
   data = {'value' : frame.values.ravel('F'),
           'variable' : np.asarray(frame.columns).repeat(N),
           'date' : np.tile(np.asarray(frame.index), K)}
   return pd.DataFrame(data, columns=['date', 'variable', 'value'])

df = unpivot(tm.makeTimeDataFrame())

df[df['variable'] == 'A']
df.pivot(index='date', columns='variable', values='value')
df['value2'] = df['value']*2
df.pivot('date', 'variable')

pivoted = df.pivot('date', 'variable')
pivoted['value2']

```

### 数据透视表

1.  pivot为不同数据类型(字符串、数字等)的数据框提供了通用的旋转功能；
2.  pivot_table函数用于对数值型数据的聚合结果进行旋转；
3.  pandas.pivot_table函数可以用来创建电子表格样式的数据透视表。

### 数据透视表

1.  pandas.pivot_table中包括一些参数：
    -   data: 一个数据框的对象；
    -   values: 一个用来聚合的列或列表；
    -   index: 一个具有相同的数据长度或列表长度的列，组，数组。在透视表索引上分
        组的键。如果传递了一个数组，则它的使用方式也适用于列值的传递；
    -   columns: 一个具有相同的数据长度或列表长度的列，组，数组。在透视表索引上
        分组的键。如果传递了一个数组，则它的使用方式也适用于列值的传递；
    -   aggfunc: 用于聚合的函数，默认为numpy.mean。
2.  从margin =True转到pivot_table，所有特殊的行和列将跨类别与部分组别聚合。

### 数据透视表

``` {.python}
df = pd.DataFrame({"A": ["foo", "foo", "foo", "foo", "foo",
                         "bar", "bar", "bar", "bar"],
                   "B": ["one", "one", "one", "two", "two",
                         "one", "one", "two", "two"],
                   "C": ["small", "large", "large", "small",
                         "small", "large", "small", "small",
                         "large"],
                   "D": [1, 2, 2, 3, 3, 4, 5, 6, 7],
                   "E": [2, 4, 5, 5, 6, 6, 8, 9, 9]})

table = pd.pivot_table(df, values='D', index=['A', 'B'],
                    columns=['C'], aggfunc=np.sum)

table = pd.pivot_table(df, values=['D', 'E'], index=['A', 'C'],
                    aggfunc={'D': np.mean,
                             'E': np.mean})

table = pd.pivot_table(df, values=['D', 'E'], index=['A', 'C'],
                    aggfunc={'D': np.mean,
                             'E': [min, max, np.mean]})
```

### 交叉表格

1.  使用交叉表函数计算两个(或更多)因子的交叉表。
2.  默认情况下，交叉表计算的是因子的频率表，除非传递了值数组和聚合函数。
3.  参数：
    -   index: 类似数组，值按行分组；
    -   columns: 类似数组，值按列分组；
    -   values: 类数组的可选值数组，根据因子进行聚合；
    -   aggfunc: 函数，可选，如果数组没有传递值，则计算频率表；
    -   rownames: 序列，默认为None，必须匹配传递的行数组数；
    -   colnames: 序列，默认为None，如果传递，则必须匹配传递的列数组数；
    -   margins: 布尔值，默认为False，添加行/列边距(小计)；
    -   normalize: 布尔值，{'all'，'index'，'columns'}，或{0,1}，默认为False。通
        过将所有值除以值的总和来标准化。

### 交叉表格

``` {.python}
foo, bar, dull, shiny, one, two = 'foo', 'bar', 'dull', 'shiny', 'one', 'two'
a = np.array([foo, foo, bar, bar, foo, foo], dtype=object)
b = np.array([one, one, two, one, two, one], dtype=object)
c = np.array([dull, dull, shiny, dull, dull, shiny], dtype=object)
pd.crosstab(a, [b, c], rownames=['a'], colnames=['b', 'c'])

df = pd.DataFrame({'A': [1, 2, 2, 2, 2], 'B': [3, 3, 4, 4, 4],
                   'C': [1, 1, np.nan, 1, 1]})
pd.crosstab(df.A, df.B)
pd.crosstab(df['A'], df['B'], normalize=True)
pd.crosstab(df['A'], df['B'], normalize='columns')
pd.crosstab(df.A, df.B, values=df.C, aggfunc=np.sum, normalize=True,
margins=True)

```

### `cut` function

1.  The cut function computes groupings for the values of the input
    array and is often used to transform continuous variables to
    discrete or categorical variables.
2.  If the bins keyword is an integer, then equal-width bins are formed.
    Alternatively we can specify custom bin-edges

### `cut` function

``` {.python}
ages = np.array([10, 15, 13, 12, 23, 25, 28, 59, 60])
pd.cut(ages, bins=3)
pd.cut(ages, bins=[0, 18, 35, 70])
```

### dummy variables: get~dummies~()

1.  get~dummies~() converts a categorical variable into a "dummy" or
    "indicator" DataFrame,
2.  for example a column in a DataFrame (a Series) which has k distinct
    values, can derive a DataFrame containing k columns of 1s and 0s
3.  get~dummies~() also accepts a DataFrame. By default all categorical
    variables (categorical in the statistical sense, those with object
    or categorical dtype) are encoded as dummy variables.
4.  control the columns that are encoded with the columns keyword.
5.  drop~first~ keyword only keep k-1 levels of a categorical variable
    to avoid collinearity.

### dummy variables: get~dummies~()

``` {.python}
df = pd.DataFrame({'key': list('bbacab'), 'data1': range(6)})
pd.get_dummies(df['key'])
dummies = pd.get_dummies(df['key'], prefix='key')

df = pd.DataFrame({'A': ['a', 'b', 'a'], 'B': ['c', 'c', 'b'],
                   'C': [1, 2, 3]})

pd.get_dummies(df['key'])
pd.get_dummies(df, columns=['A'])
```

### 其他函数

1.  stack unstack
2.  melt
3.  wide~tolong~

# Categoricals

### 创建分类

1.  By specifying dtype=\"category\" when constructing a Series
2.  pandas can include categorical data in a DataFrame by convert
3.  Rename the categories to more meaningful names (assigning to
    Series.cat.categories
4.  Reorder the categories and simultaneously add the missing categories
5.  Sorting is per order in the categories, not lexical order.
6.  Grouping by a categorical column shows also empty categories.

### 例子

``` {.python}
s = pd.Series(["a","b","c","a"], dtype="category")
df = pd.DataFrame({"id":[1,2,3,4,5,6], "raw_grade":['a', 'b', 'b', 'a', 'a', 'e']})
df["grade"] = df["raw_grade"].astype("category")
df["grade"].cat.categories = ["very good", "good", "very bad"]
df["grade"] = df["grade"].cat.set_categories(["very bad", "bad", "medium", "good", "very good"])

df.sort_values(by="grade")
df.groupby("grade").size()

```

# Plotting

### `plot()`

1.  The plot method on Series and DataFrame is just a simple wrapper
    around plt.plot()

```{=html}
<!-- -->
```
1.  On DataFrame, plot() is a convenience to plot all of the columns
    with labels
2.  plot one column versus another using the x and y keywords in plot()

``` {.python}
ts = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2000',periods=1000))
ts = ts.cumsum()
ts.plot()

df3 = pd.DataFrame(np.random.randn(1000, 2), columns=['B', 'C']).cumsum()
df3['A'] = pd.Series(list(range(len(df))))

df3.plot(x='A', y='B')
```

### Other Plots

1.  Plotting methods allow for a handful of plot styles other than the
    default Line plot. These methods can be provided as the kind keyword
    argument to plot(). These include:
    -   'bar' or 'barh' for bar plots
    -   'hist' for histogram
    -   'box' for boxplot
    -   'kde' or \'density\' for density plots
    -   'area' for area plots
    -   'scatter' for scatter plots
    -   'hexbin' for hexagonal bin plots
    -   'pie' for pie plots

### 其他画图函数

1.  These functions can be imported from pandas.plotting and take a
    Series or DataFrame as an argument.
2.  Scatter Matrix Plot： pandas.plotting.scatter~matrix~
3.  create density plots using the Series.plot.kde() and
    DataFrame.plot.kde() methods.
4.  Andrews curves allow one to plot multivariate data as a large number
    of curves that are created using the attributes of samples as
    coefficients for Fourier series: andrews~curves~

### 其他画图函数

1.  [@5]Parallel coordinates is a plotting technique for plotting
    multivariate data. It allows one to see clusters in data and to
    estimate other statistics visually: parallel~coordinates~
2.  Lag plots are used to check if a data set or time series is random:
    lag~plot~
3.  Autocorrelation Plot: autocorrelation~plot~
4.  Bootstrap plots are used to visually assess the uncertainty of a
    statistic, such as mean, median, midrange, etc: bootstrap~plot~
5.  RadViz is a way of visualizing multi-variate data: radviz

<!-- # 参考文献 -->
[//]: # (\bibliography{Bibfile})
